\providecommand\numberofexercises{}
\XSIM{solution-body}{exercise-1=={\par Let \par \[ \lambda _{\text {max}} = \max _j \left | \sum _{i=1}^{n} y_i x_{ij} \right | \] \par \[ \forall j, \quad \lambda _{\text {max}} |\beta _j| \geq \beta _j \left | \sum _{i=1}^{n} y_i x_{ij} \right | \] \par \[ \lambda _{\text {max}} \|\beta \|_1 \geq \sum _{j=1}^{p} |\beta _j| \lambda _{\text {max}} \geq y^T X \beta \] \par \[ \lambda _{\text {max}} \|\beta \|_1 + \frac {1}{2} \beta ^T X^T X \beta - y^T X \beta \geq 0 \] \par since $X^T X$ is symetric semi-definite positive. \par and \[ f(\beta ) \geq f(0) \quad \Rightarrow \quad \arg \min f(\beta ) = 0^p \quad \text {si} \quad \lambda > \lambda _{\text {max}} \] \par \[ \boxed {\lambda _{\text {max}} = \max _j \left | \sum _{i=1}^{n} y_i x_{ij} \right | = \|X^Ty \|_{\infty } } \] \par \par }||exercise-2=={\par \begin {itemize} \item Let's start by writing the vector $\sum _{k=1}^{K} \mathbf {z}_k * \mathbf {d}_k$ as $\mathbf {D}\mathbf {Z}$ where $\mathbf {D} \in \mathbb {R}^{N \times KN}$ and $\mathbf {Z} \in \mathbb {R}^{KN}$. \par We have, for element $i \in [1,N]$: \[ \left ( \sum _{k=1}^{K} \mathbf {z}_k * \mathbf {d}_k \right )_i = \sum _{k=1}^{K} \sum _{j=1}^{N} \mathbf {z}_k(j) \, \mathbf {d}_k(i-j) \tag {$\star $} \] \par such that $\mathbf {z}_k(j) = \begin {cases} (\mathbf {z}_k)_j & \text {if } 1 \leq j \leq N-L+1 \\ 0 & \text {otherwise} \end {cases}$ \par and $\mathbf {d}_k(i-j) = \begin {cases} (\mathbf {d}_k)_{i-j} & \text {if } 1 \leq i-j \leq L \\ 0 & \text {otherwise} \end {cases}$ \par And, for element $i \in [1,N]$: \[ \left (\mathbf {DZ} \right )_i = \sum _{j=1}^{KN} \mathbf {D}_{ij}\mathbf {Z}_j \tag {$\star $} \] \par This gives us $\mathbf {Z} = \left (\mathbf {z}_1, 0, \dots , 0, \mathbf {z}_2, 0, \dots , 0, \dots , \mathbf {z}_K, 0, \dots , 0\right ) \in \mathbb {R}^{KN}.$ \par And $\mathbf {D} = (\mathbf {D}_1, \mathbf {D}_2, \dots , \mathbf {D}_K)$ where for each $k \in [1,K]:$ \[ (\mathbf {D}_k)_{ij} = \mathbf {d}_k(i-j) \] \par With these notations, the sparse coding problem is in fact a lasso regression. \par \item Using Q.1 we have $\boxed {\lambda _{\max } = \norm {\mathbf {D}^T\mathbf {x}}_\infty }$ \par \end {itemize} \par \par }||exercise-3=={Soit $\tau \in \mathbb {Z}$ \par \[ \gamma (\tau ) = \mathbb {E}\left (X_n X_{n+\tau }\right ) = \begin {cases} \sigma ^2 & \text {si } \tau = 0 \\ 0 & \text {sinon} \end {cases} \] \par \[ S(f) = \sigma ^2 \] \par }||exercise-4=={\par \[ \mathbb {E}\left [\hat {\gamma }(\tau )\right ] = \frac {1}{N} \sum _{n=0}^{N-\tau -1} \mathbb {E}\left (X_n X_{n+\tau }\right ) \] \par \[ = \gamma (\tau ) \frac {N - \tau }{N} \] \par \[ \overset {N \to \infty }{\longrightarrow } \gamma (\tau ) \] \par \[ \hat {\gamma }_{\text {unbiased}}(\tau ) = \frac {1}{N-\tau } \sum _{n=0}^{N-\tau -1} X_n X_{n+\tau } \] \par }||exercise-5=={\par Let $f \in [0, N/2]$, \begin {align*} | J(f_k) |^2 &= \frac {1}{N} \sum _{n=0}^{N-1} \sum _{p=0}^{N-1} X_n X_p e^{-2i\pi f(n-p)/f_s} \\ &= \frac {1}{N} \sum _{n=0}^{N-1} \sum _{\tau =-(N-1)}^{N-1} X_n X_{n-\tau } e^{-2i\pi f\tau /f_s } \quad (\tau = n - p) \\ &= \frac {1}{N} \sum _{\tau =-(N-1)}^{N-1} \sum _{n=\tau }^{N-1} X_n X_{n-\tau } e^{-2i\pi f\tau /f_s } \\ &= \frac {1}{N} \sum _{\tau =-(N-1)}^{N-1} \left ( \sum _{n=0}^{N-\tau -1} X_n X_{n+\tau } \right ) e^{-2i\pi f\tau /f_s} \\ &= \sum _{\tau =-(N-1)}^{N-1} \hat {\gamma }^{(N)}(\tau ) e^{-2i\pi f\tau /f_s } \end {align*} \par Now, as \par \[ f^{(N)} \xrightarrow {N \to \infty } f: \] \par \[ \mathbb {E} [\hat {\gamma }^{(N)}(\tau )] \xrightarrow {N \to \infty } \gamma (\tau ) \] and \[ \mathbb {E} \left [ \hat {\gamma }^{(N)}(\tau ) e^{-2i\pi (f\tau /f_s)} \right ]^2 \leq |\gamma (\tau )|^2 \] \par Thus: \[ \mathbb {E} \left ( | J(f^N) |^2 \right ) \xrightarrow {N \to \infty } S(f) \] \par }||exercise-6=={\textbf {Autocovariance:} At $\tau = 0$, the average sample covariance equals the standard deviation, while for $\tau \neq 0$, it approaches 0. The standard deviation decreases with more generations, and the sample covariance seems to tend to 0 for $\tau \neq 0$, consistent with $\hat {\gamma }(\tau )$ being an asymptotically unbiased estimator of $\gamma (\tau )$. \par \textbf {Periodogram:} The periodogram shows random, unstable behavior without a clear trend, and its variance does not decrease as the number of samples increases. \par \par }||exercise-7=={\par Let $\tau > 0$. \par \[ \text {Var}(\hat {\gamma }(\tau )) = \text {Cov}(\hat {\gamma }(\tau ), \hat {\gamma }(\tau )) \] \par \[ = \frac {1}{N^2} \sum _{m=0}^{N-\tau -1} \sum _{p=0}^{N-\tau -1} \text {Cov}(X_m X_{m+\tau }, X_p X_{p+\tau }) \] \par \[ = \frac {1}{N^2} \sum _{m=0}^{N-\tau -1} \sum _{p=0}^{N-\tau -1} \big [ \mathbb {E}(X_m X_{m+\tau } X_p X_{p+\tau }) - \mathbb {E}(X_m X_{m+\tau }) \mathbb {E}(X_p X_{p+\tau }) \big ] \] \par \[ = \frac {1}{N^2} \sum _{m=0}^{N-\tau -1} \sum _{p=0}^{N-\tau -1} \big [ \mathbb {E}(X_m X_p) \mathbb {E}(X_{m+\tau } X_{p+\tau }) + \mathbb {E}(X_m X_{p+\tau }) \mathbb {E}(X_{m+\tau } X_p) - \mathbb {E}(X_m X_{m+\tau }) \mathbb {E}(X_p X_{p+\tau }) \big ] \] \par \[ = \frac {1}{N^2} \sum _{m=0}^{N-\tau -1} \sum _{p=0}^{N-\tau -1} \big [ \gamma (m-p)^2 + \gamma (m-p-\tau )\gamma (m-p+\tau ) \big ] \] \par Let $k = m - p$: \par \[ = \frac {1}{N^2} \sum _{m=0}^{N-\tau -1} \sum _{k=-(N-\tau -1)}^{N-\tau -1} \gamma (k)^2 + \gamma (k-\tau )\gamma (k+\tau ) \] \par \[ = \frac {1}{N} \sum _{k=-(N-\tau -1)}^{N-\tau -1} \left ( \gamma (k)^2 + \gamma (k-\tau ) \gamma (k+\tau ) \right ) \left ( 1 - \frac {\tau + |k|}{N} \right ) \] \par Since \[ \sum _{t \in \mathbb {Z}} \gamma (t)^2 < \infty \] \par \[ \text {Var}(\hat {\gamma }(\tau )) \leq \frac {1}{N} \left ( \sum _{k \in \mathbb {Z}} \gamma (k)^2 + \sum _{k \in \mathbb {Z}} \gamma (k - \tau ) \gamma (k + \tau ) \right ) \] \par \[ \leq \frac {2}{N} \sum _{k \in \mathbb {Z}} \gamma (k)^2 \rightarrow 0 \quad \text {as} \quad N \rightarrow \infty \] \par The last inequality and convergence of the second sum in ensured by Cauchy-Schwartz inequality. \par With Chebyshev's inequality, we conclude that: \par \[ \hat {\gamma }(\tau ) \xrightarrow {P} \gamma (\tau ) \quad \text {as} \quad N \rightarrow \infty \] \par }||exercise-8=={\par The $X_m$ are independent. Thus, \par \[ \mathbb {E}[A(f_l)] = \mathbb {E}[B(f_l)] = 0 \] \par \[ \text {Var}(A(f_l)) = \sum _{m=0}^{N-1} \sigma ^2 \cos ^2(-2 \pi f_l m / f_s) = \sigma ^2 \frac {N}{2} \] \par \[ \text {Var}(B(f_l)) = \sum _{m=0}^{N-1} \sigma ^2 \sin ^2(-2 \pi f_l m / f_s) = \sigma ^2 \frac {N}{2} \] \par \[ A(f_l) \sim \mathcal {N} \left (0, \frac {\sigma ^2 N}{2} \right ), \quad B(f_l) \sim \mathcal {N} \left (0, \frac {\sigma ^2 N}{2} \right ) \] \par \[ \mathbb {E} \left [ | I(f_l) |^2 \right ] = \frac {4}{N} \left [ A^2(f_l) + B^2(f_l) \right ] \] \par And \par \[ \frac {A^2(f_l)}{\sigma ^2 \frac {N}{2}} \sim \chi ^2(1), \quad \frac {B^2(f_l)}{\sigma ^2 \frac {N}{2}} \sim \chi ^2(1) \] \par Thus, \par \[ 2 \frac {|I(f_l)|^2}{\sigma ^2} \sim \chi ^2(2) \] \par \[ \text {Var} \left [ \mathbb {E} \left ( | I(f_l) |^2 \right ) \right ] = \frac {\sigma ^4}{4} \text {Var} \left ( 2 \frac {|I(f_l)|^2}{\sigma ^2} \right ) \] \par \[ = \sigma ^4 \] \par \[ \mathbb {E}\left ( \left | J(f_k) \right |^2 \left | J(f_\ell ) \right |^2 \right ) = \frac {1}{N^2} \sum _{n=0}^{N-1} \sum _{m=0}^{N-1} \sum _{p=0}^{N-1} \sum _{q=0}^{N-1} \mathbb {E}\left ( X_n X_{m} X_p X_{q} \right ) e^{-2 i \pi \frac {(k(n-p) + \ell (m-q))}{N}f_s} \] \par \[ = \frac {1}{N^2} \sum _{n=0}^{N-1} \sum _{m=0}^{N-1} \sum _{p=0}^{N-1} \sum _{q=0}^{N-1} \left ( \mathbb {E}(X_n X_{m}) \mathbb {E}(X_p X_{q}) + \mathbb {E}(X_n X_p) \mathbb {E}(X_{m} X_{q}) - \mathbb {E}(X_n X_{q}) \mathbb {E}(X_{m} X_p) \right ) e^{-2 i \pi \frac {(k(n-p) + \ell (m-q))}{N}f_s} \] \par \[ = \sigma ^4 + \frac {1}{N^2} \sum _{n=0}^{N-1} \sum _{p=0}^{N-1} \sigma ^4 e^{-2 i \pi \frac {(k+\ell )(n-p)}{N}f_s} + \frac {1}{N^2} \sum _{n=0}^{N-1} \sum _{m=0}^{N-1} \sigma ^4 e^{-2 i \pi \frac {(k-\ell )(n-m)}{N}f_s} \] \par \[= \left \{ \begin {array}{ll} 2\sigma ^4 & \text {if } k = \ell \\ \sigma ^4 & \text {if } k \neq \ell \end {array} \right . \] \par Thus, \[ \text {cov} \left ( \left | J(f_k) \right |^2 \left | J(f_\ell ) \right |^2 \right ) = \left \{ \begin {array}{ll} \sigma ^4 & \text {if } k = \ell \\ 0 & \text {if } k \neq \ell \end {array} \right . \] \par If k in different from l, there is no correlation between the terms of the periodogram, thus the erratic behavior. \par }||exercise-9=={\par \begin {figure} \centering \begin {minipage}[t]{0.3\textwidth } \centerline {\includegraphics [width=\textwidth ]{Barlett200}} \centerline {Periodogram ($N=200$)} \end {minipage} \begin {minipage}[t]{0.3\textwidth } \centerline {\includegraphics [width=\textwidth ]{Barlett500}} \centerline {Periodogram ($N=500$)} \end {minipage} \begin {minipage}[t]{0.3\textwidth } \centerline {\includegraphics [width=\textwidth ]{Barlett1000}} \centerline {Periodogram ($N=1000$)} \end {minipage} \vskip 1em \caption {Barlett's periodograms of a Gaussian white noise (see Question~\ref {q:barlett}).} \label {fig:barlett} \end {figure} \par We observe that compare to Question 6, the standard deviation with this method is smaller, which is in accordance with the principle of the method that divides the variance by a factor 5 and the the standard deviation by a factor $\sqrt {5}$. \par }||exercise-10=={\par The model is trained using 5-fold cross-validation, with the optimal number of neighbors determined to be 5. For label 1, the F-score achieved is 0.87 on the training set, while it drops to 0.45 on the test set. For label 0, the F-score achieved is 0.88 on the training set, while it drops to 0.09 on the test set. \par \par }||exercise-11=={\begin {figure} \centering \begin {minipage}[t]{\textwidth } \centerline {\includegraphics [width=0.6\textwidth ]{badly_healthy}} \centerline {Badly classified healthy step} \end {minipage} \vskip 1em \begin {minipage}[t]{\textwidth } \centerline {\includegraphics [width=0.6\textwidth ]{badly-non-healthy}} \centerline {Badly classified non-healthy step} \end {minipage} \vskip 1em \caption {Examples of badly classified steps (see Question~\ref {q:class-errors}).} \label {fig:class-errors} \end {figure}}}
\XSIM{exercise-body}{exercise-1=={Consider the following Lasso regression: \begin {equation}\label {eq:lasso} \min _{\beta \in \RR ^p} \frac {1}{2}\norm {y-X\beta }^2_2 \quad + \quad \lambda \norm {\beta }_1 \end {equation} where $y\in \RR ^n$ is the response vector, $X\in \RR ^{n\times p}$ the design matrix, $\beta \in \RR ^p$ the vector of regressors and $\lambda >0$ the smoothing parameter. \par Show that there exists $\lambda _{\max }$ such that the minimizer of~\eqref {eq:lasso} is $\mathbf {0}_p$ (a $p$-dimensional vector of zeros) for any $\lambda > \lambda _{\max }$.}||exercise-2=={For a univariate signal $\mathbf {x}\in \mathbb {R}^n$ with $n$ samples, the convolutional dictionary learning task amounts to solving the following optimization problem: \par \begin {equation} \min _{(\mathbf {d}_k)_k, (\mathbf {z}_k)_k \\ \norm {\mathbf {d}_k}_2^2\leq 1} \quad \norm {\mathbf {x} - \sum _{k=1}^K \mathbf {z}_k * \mathbf {d}_k }^2_2 \quad + \quad \lambda \sum _{k=1}^K \norm {\mathbf {z}_k}_1 \end {equation} \par where $\mathbf {d}_k\in \mathbb {R}^L$ are the $K$ dictionary atoms (patterns), $\mathbf {z}_k\in \mathbb {R}^{N-L+1}$ are activations signals, and $\lambda >0$ is the smoothing parameter. \par Show that \begin {itemize} \item for a fixed dictionary, the sparse coding problem is a lasso regression (explicit the response vector and the design matrix); \item for a fixed dictionary, there exists $\lambda _{\max }$ (which depends on the dictionary) such that the sparse codes are only 0 for any $\lambda > \lambda _{\max }$. \end {itemize}}||exercise-3=={In this question, let $X_n$ ($n=0,\dots ,N-1)$ be a Gaussian white noise. \par \begin {itemize} \item Calculate the associated autocovariance function and power spectrum. (By analogy with the light, this process is called ``white'' because of the particular form of its power spectrum.) \end {itemize} \par }||exercise-4=={A natural estimator for the autocorrelation function is the sample autocovariance \begin {equation} \hat {\gamma }(\tau ) := (1/N) \sum _{n=0}^{N-\tau -1} X_n X_{n+\tau } \end {equation} for $\tau =0,1,\dots ,N-1$ and $\hat {\gamma }(\tau ):=\hat {\gamma }(-\tau )$ for $\tau =-(N-1),\dots ,-1$. \begin {itemize} \item Show that $\hat {\gamma }(\tau )$ is a biased estimator of $\gamma (\tau )$ but asymptotically unbiased. What would be a simple way to de-bias this estimator? \end {itemize} \par }||exercise-5=={Define the discrete Fourier transform of the random process $\{X_n\}_n$ by \begin {equation} J(f) := (1/\sqrt {N})\sum _{n=0}^{N-1} X_n e^{-2\pi \iu f n/f_s} \end {equation} The \textit {periodogram} is the collection of values $|J(f_0)|^2$, $|J(f_1)|^2$, \dots , $|J(f_{N/2})|^2$ where $f_k = f_s k/N$. (They can be efficiently computed using the Fast Fourier Transform.) \begin {itemize} \item Write $|J(f_k)|^2$ as a function of the sample autocovariances. \item For a frequency $f$, define $f^{(N)}$ the closest Fourier frequency $f_k$ to $f$. Show that $|J(f^{(N)})|^2$ is an asymptotically unbiased estimator of $S(f)$ for $f>0$. \end {itemize}}||exercise-6=={\label {ex:wn-exp} In this question, let $X_n$ ($n=0,\dots ,N-1)$ be a Gaussian white noise with variance $\sigma ^2=1$ and set the sampling frequency to $f_s=1$ Hz \begin {itemize} \item For $N\in \{200, 500, 1000\}$, compute the \textit {sample autocovariances} ($\hat {\gamma }(\tau )$ vs $\tau $) for 100 simulations of $X$. Plot the average value as well as the average $\pm $, the standard deviation. What do you observe? \item For $N\in \{200, 500, 1000\}$, compute the \textit {periodogram} ($|J(f_k)|^2$ vs $f_k$) for 100 simulations of $X$. Plot the average value as well as the average $\pm $, the standard deviation. What do you observe? \end {itemize} Add your plots to Figure~\ref {fig:wn-exp}. \par \begin {figure} \centering \begin {minipage}[t]{0.3\textwidth } \centerline {\includegraphics [width=\textwidth ]{autocov200}} \centerline {Autocovariance ($N=200$)} \end {minipage} \begin {minipage}[t]{0.3\textwidth } \centerline {\includegraphics [width=\textwidth ]{autocov500}} \centerline {Autocovariance ($N=500$)} \end {minipage} \begin {minipage}[t]{0.3\textwidth } \centerline {\includegraphics [width=\textwidth ]{autocov1000}} \centerline {Autocovariance ($N=1000$)} \end {minipage} \vskip 1em \begin {minipage}[t]{0.3\textwidth } \centerline {\includegraphics [width=\textwidth ]{periodogram200}} \centerline {Periodogram ($N=200$)} \end {minipage} \begin {minipage}[t]{0.3\textwidth } \centerline {\includegraphics [width=\textwidth ]{periodogram500}} \centerline {Periodogram ($N=500$)} \end {minipage} \begin {minipage}[t]{0.3\textwidth } \centerline {\includegraphics [width=\textwidth ]{periodogram1000}} \centerline {Periodogram ($N=1000$)} \end {minipage} \vskip 1em \caption {Autocovariances and periodograms of a Gaussian white noise (see Question~\ref {ex:wn-exp}).} \label {fig:wn-exp} \end {figure} \par }||exercise-7=={We want to show that the estimator $\hat {\gamma }(\tau )$ is consistent, \ie it converges in probability when the number $N$ of samples grows to $\infty $ to the true value ${\gamma }(\tau )$. In this question, assume that $X$ is a wide-sense stationary \textit {Gaussian} process. \begin {itemize} \item Show that for $\tau >0$ \begin {equation} \text {var}(\hat {\gamma }(\tau )) = (1/N) \sum _{n=-(N-\tau -1)}^{n=N-\tau -1} \left (1 - \frac {\tau + |n|}{N}\right ) \left [\gamma ^2(n) + \gamma (n-\tau )\gamma (n+\tau )\right ]. \end {equation} (Hint: if $\{Y_1, Y_2, Y_3, Y_4\}$ are four centered jointly Gaussian variables, then $\mathbb {E}[Y_1 Y_2 Y_3 Y_4] = \mathbb {E}[Y_1 Y_2]\mathbb {E}[Y_3 Y_4] + \mathbb {E}[Y_1 Y_3]\mathbb {E}[Y_2 Y_4] + \mathbb {E}[Y_1 Y_4]\mathbb {E}[Y_2 Y_3]$.) \item Conclude that $\hat {\gamma }(\tau )$ is consistent. \end {itemize}}||exercise-8=={Assume that $X$ is a Gaussian white noise (variance $\sigma ^2$) and let $A(f):=\sum _{n=0}^{N-1} X_n \cos (-2\pi f n/f_s$ and $B(f):=\sum _{n=0}^{N-1} X_n \sin (-2\pi f n/f_s$. Observe that $J(f) = (1/N) (A(f) + \iu B(f))$. \begin {itemize} \item Derive the mean and variance of $A(f)$ and $B(f)$ for $f=f_0, f_1,\dots , f_{N/2}$ where $f_k=f_s k/N$. \item What is the distribution of the periodogram values $|J(f_0)|^2$, $|J(f_1)|^2$, \dots , $|J(f_{N/2})|^2$. \item What is the variance of the $|J(f_k)|^2$? Conclude that the periodogram is not consistent. \item Explain the erratic behavior of the periodogram in Question~\ref {ex:wn-exp} by looking at the covariance between the $|J(f_k)|^2$. \end {itemize} \par }||exercise-9=={\label {q:barlett} As seen in the previous question, the problem with the periodogram is the fact that its variance does not decrease with the sample size. A simple procedure to obtain a consistent estimate is to divide the signal into $K$ sections of equal durations, compute a periodogram on each section, and average them. Provided the sections are independent, this has the effect of dividing the variance by $K$. This procedure is known as Bartlett's procedure. \begin {itemize} \item Rerun the experiment of Question~\ref {ex:wn-exp}, but replace the periodogram by Barlett's estimate (set $K=5$). What do you observe? \end {itemize} Add your plots to Figure~\ref {fig:barlett}.}||exercise-10=={Combine the DTW and a k-neighbors classifier to classify each step. Find the optimal number of neighbors with 5-fold cross-validation and report the optimal number of neighbors and the associated F-score. Comment briefly.}||exercise-11=={\label {q:class-errors} Display on Figure~\ref {fig:class-errors} a badly classified step from each class (healthy/non-healthy).}}
\XSIM{goal}{exercise}{points}{0}
\XSIM{totalgoal}{points}{0}
\XSIM{goal}{exercise}{bonus-points}{0}
\XSIM{totalgoal}{bonus-points}{0}
\XSIM{order}{1||2||3||4||5||6||7||8||9||10||11}
\XSIM{use}{}
\XSIM{use!}{}
\XSIM{used}{exercise-1=={true}||exercise-2=={true}||exercise-3=={true}||exercise-4=={true}||exercise-5=={true}||exercise-6=={true}||exercise-7=={true}||exercise-8=={true}||exercise-9=={true}||exercise-10=={true}||exercise-11=={true}}
\XSIM{print}{}
\XSIM{print!}{}
\XSIM{printed}{exercise-1=={true}||exercise-2=={true}||exercise-3=={true}||exercise-4=={true}||exercise-5=={true}||exercise-6=={true}||exercise-7=={true}||exercise-8=={true}||exercise-9=={true}||exercise-10=={true}||exercise-11=={true}}
\XSIM{total-number}{11}
\XSIM{exercise}{11}
\XSIM{types}{exercise}
\XSIM{idtypes}{1=={exercise}||2=={exercise}||3=={exercise}||4=={exercise}||5=={exercise}||6=={exercise}||7=={exercise}||8=={exercise}||9=={exercise}||10=={exercise}||11=={exercise}}
\XSIM{collections}{exercise-1=={all exercises}||exercise-2=={all exercises}||exercise-3=={all exercises}||exercise-4=={all exercises}||exercise-5=={all exercises}||exercise-6=={all exercises}||exercise-7=={all exercises}||exercise-8=={all exercises}||exercise-9=={all exercises}||exercise-10=={all exercises}||exercise-11=={all exercises}}
\XSIM{collection:all exercises}{exercise-1||exercise-2||exercise-3||exercise-4||exercise-5||exercise-6||exercise-7||exercise-8||exercise-9||exercise-10||exercise-11}
\setcounter{totalexerciseinall exercises}{11}
\XSIM{id}{exercise-1=={1}||exercise-2=={2}||exercise-3=={3}||exercise-4=={4}||exercise-5=={5}||exercise-6=={6}||exercise-7=={7}||exercise-8=={8}||exercise-9=={9}||exercise-10=={10}||exercise-11=={11}}
\XSIM{ID}{exercise-1=={1}||exercise-2=={2}||exercise-3=={3}||exercise-4=={4}||exercise-5=={5}||exercise-6=={6}||exercise-7=={7}||exercise-8=={8}||exercise-9=={9}||exercise-10=={10}||exercise-11=={11}}
\XSIM{counter}{exercise-1=={1}||exercise-2=={2}||exercise-3=={3}||exercise-4=={4}||exercise-5=={5}||exercise-6=={6}||exercise-7=={7}||exercise-8=={8}||exercise-9=={9}||exercise-10=={10}||exercise-11=={11}}
\XSIM{counter-value}{exercise-1=={1}||exercise-2=={2}||exercise-3=={3}||exercise-4=={4}||exercise-5=={5}||exercise-6=={6}||exercise-7=={7}||exercise-8=={8}||exercise-9=={9}||exercise-10=={10}||exercise-11=={11}}
\XSIM{solution}{}
\XSIM{section-value}{exercise-1=={2}||exercise-2=={2}||exercise-3=={3}||exercise-4=={3}||exercise-5=={3}||exercise-6=={3}||exercise-7=={3}||exercise-8=={3}||exercise-9=={3}||exercise-10=={4}||exercise-11=={4}}
\XSIM{section}{exercise-1=={2}||exercise-2=={2}||exercise-3=={3}||exercise-4=={3}||exercise-5=={3}||exercise-6=={3}||exercise-7=={3}||exercise-8=={3}||exercise-9=={3}||exercise-10=={4}||exercise-11=={4}}
\XSIM{sectioning}{exercise-1=={{0}{2}{0}{0}{0}}||exercise-2=={{0}{2}{0}{0}{0}}||exercise-3=={{0}{3}{0}{0}{0}}||exercise-4=={{0}{3}{0}{0}{0}}||exercise-5=={{0}{3}{0}{0}{0}}||exercise-6=={{0}{3}{0}{0}{0}}||exercise-7=={{0}{3}{0}{0}{0}}||exercise-8=={{0}{3}{0}{0}{0}}||exercise-9=={{0}{3}{0}{0}{0}}||exercise-10=={{0}{4}{2}{0}{0}}||exercise-11=={{0}{4}{2}{0}{0}}}
\XSIM{subtitle}{}
\XSIM{points}{}
\XSIM{bonus-points}{}
\XSIM{page-value}{exercise-1=={1}||exercise-2=={2}||exercise-3=={3}||exercise-4=={4}||exercise-5=={4}||exercise-6=={5}||exercise-7=={6}||exercise-8=={8}||exercise-9=={9}||exercise-10=={10}||exercise-11=={12}}
\XSIM{page}{exercise-1=={1}||exercise-2=={2}||exercise-3=={3}||exercise-4=={4}||exercise-5=={4}||exercise-6=={5}||exercise-7=={6}||exercise-8=={8}||exercise-9=={9}||exercise-10=={10}||exercise-11=={12}}
\XSIM{tags}{}
\XSIM{topics}{}
\XSIM{userpoints}{}
\XSIM{bodypoints}{}
\XSIM{userbonus-points}{}
\XSIM{bodybonus-points}{}
